{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Bloomberg GPT** - A RAG system with Generative AI"
      ],
      "metadata": {
        "id": "R68Lz_rJPbRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**\n",
        "\n",
        "I used LangChain to:\n",
        "- Open pdf and turn into text pages,\n",
        "- Create the chunks and define chunks parameters\n",
        "- Store the already embedded (with openai) chunks in a vector database (Chroma)\n",
        "- Retrieve the chunks for the vector store to answer the questions\n",
        "\n",
        "I used OpenAI to:\n",
        "- Create the chunks embeddings\n",
        "- Create the embedding of the question to look for similar chunks (usign cosine similarity)\n",
        "- Acess LLMs to take the question and generate an answer"
      ],
      "metadata": {
        "id": "w7vk61JWADf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai chromadb tiktoken pypdf sentence_transformers -U langchain-community --quiet"
      ],
      "metadata": {
        "id": "pDqvYgcVB_QW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "P9UOMe10Pdqg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "njUxkcmLuB9j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Functions to prepare the text to make the questions**\n",
        "\n",
        "load_pdf -  to open the pdf and extract the text\n",
        "\n",
        "create_smart_chunks - divide the whole content into chunks, with overlapping to help maintain context\n",
        "\n",
        "store_in_vector_db - it acess it chunk created, embed it with openai, and store it on a vector database\n",
        "\n",
        "process_pdf_for_rag - run all the above functions sequentially"
      ],
      "metadata": {
        "id": "jqZTIek6uKN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(pdf_path):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    pages = loader.load()\n",
        "    return pages\n",
        "\n",
        "def create_smart_chunks(pages, chunk_size=800, chunk_overlap=200):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "    #print(f\"Number of chunks: {len(chunks)}\")\n",
        "\n",
        "    # for i, chunk in enumerate(chunks[:30]):\n",
        "    #     print(f\"Chunk {i + 1}:\")\n",
        "    #     print(chunk)\n",
        "    #     print(\"-\" * 80)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def store_in_vector_db(chunks, vector_db_path='./chroma_store'):\n",
        "\n",
        "    embedding_model = OpenAIEmbeddings(model='text-embedding-3-small', disallowed_special=())  # Allow all special tokens\n",
        "    vectorstore = Chroma.from_documents(chunks, embedding=embedding_model, persist_directory=vector_db_path)\n",
        "    vectorstore.persist()\n",
        "    return vectorstore\n",
        "\n",
        "def process_pdf_for_rag(pdf_path):\n",
        "\n",
        "    pages = load_pdf(pdf_path)\n",
        "    chunks = create_smart_chunks(pages)\n",
        "    vectorstore = store_in_vector_db(chunks)\n",
        "\n",
        "    return vectorstore\n"
      ],
      "metadata": {
        "id": "CYzGf4fuWxXM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer Generation**\n",
        "\n",
        "- Determine the file utilized (BloombergGPT pdf)\n",
        "\n",
        "- Execute the above functions to prepara the data\n",
        "\n",
        "- Call a LLM model (gpt-4o-mini, in the case) and set it up (acess to vector store, number of chunks retrieved to build the answer, etc)\n",
        "\n",
        "- Frame the questions and generate the answer\n",
        "\n",
        "- See the similiraty score for each chunk retrieved for that question"
      ],
      "metadata": {
        "id": "vDdoDj7Nuh1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE\n",
        "pdf_path = \"/content/bloomberggpt.pdf\"\n",
        "\n",
        "# VECTOR DATABASE\n",
        "vectorstore = process_pdf_for_rag(pdf_path)\n",
        "\n",
        "# Question and Answers\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "                    llm,\n",
        "                    chain_type=\"stuff\",\n",
        "                    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 20}),\n",
        "                    return_source_documents=True,\n",
        "                    chain_type_kwargs={\"verbose\": False})\n",
        "\n",
        "question = \"summarize the BloombergGPT article\"\n",
        "answer = qa_chain.invoke(input=question)['result']\n",
        "answer\n",
        "\n",
        "#number of chunks\n",
        "pages = load_pdf(pdf_path)\n",
        "chunks = create_smart_chunks(pages)\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "\n",
        "# # Similarity of retrieved chunks\n",
        "# chunk_and_score = vectorstore.similarity_search_with_score(question, k=3)\n",
        "# # print(chunk_and_score)\n",
        "\n",
        "# for chunk, score in chunk_and_score:\n",
        "#   print(f\"Text: '{chunk.page_content}'\")\n",
        "#   print()\n",
        "#   print(f\"Score: {score}\")\n",
        "#   print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "mnvNmAvFuggN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1958ac4a-f52f-47e4-be75-099beb99fc75"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks stored: 362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experiments: Questions to test the model**\n",
        "\n",
        "I have set some questions and defined the ideal answer/context for these questions to be able to test the model's accuracy. The questions/answers pairs were separated in 2 categories:\n",
        "\n",
        "- General content questions: to test the ability to answer general content info\n",
        "\n",
        "- Tables' Content questions: to test the ability to answer info from the tables\n"
      ],
      "metadata": {
        "id": "MsuxWfJYVKwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions"
      ],
      "metadata": {
        "id": "pq1fbqGeWtlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "general_content_questions = [\n",
        "  \"Summarize the BloombergGPT article\",\n",
        "  \"Explain BloombergGPT model architecture. in plain text, no formulas\",\n",
        "  \"What is BloombergGPT and why was it developed?\",\n",
        "  \"Tell me about BloombergGPT hardware stack\"\n",
        "  #\"How is the dataset for BloombergGPT different from general-purpose LLM datasets?\",\n",
        "  #\"What are the main financial tasks that BloombergGPT is optimized for?\",\n",
        "  #\"What were some of the challenges encountered during the training of BloombergGPT?\",\n",
        "  #\"What are the key benefits of using a domain-specific model like BloombergGPT in the financial industry?\",\n",
        "  #\"How does BloombergGPT perform in financial tasks compared to other models like GPT-3?\",\n",
        "  #\"What are the top three data sources by token contribution\"\n",
        "  ]\n",
        "\n",
        "# table_questions = [\n",
        "#   \"According to Table 1, what percentage of BloombergGPT's training data comes from public datasets?\",\n",
        "#   \"Which dataset contributes the highest number of tokens in the financial-specific data as shown in Table 2?\",\n",
        "#   \"What are the top three data sources by token contribution in the public datasets, as per Table 1?\",\n",
        "#   \"From Table 3, how does BloombergGPT's tokenizer compare to that of GPT-NeoX in terms of tokens in 'The Pile' dataset?\",\n",
        "#   \"What is the total percentage of tokens from financial-specific datasets used in training BloombergGPT, according to Table 1?\"\n",
        "#   ]"
      ],
      "metadata": {
        "id": "8sJ4Bd51NreE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reference Answers/Context"
      ],
      "metadata": {
        "id": "27ghKSWbWxG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_general_content_answers = [\n",
        "    \"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg’s extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on stan- dard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our model- ing choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.\",\n",
        "    \"Our model is a decoder-only causal language model based on BLOOM (Scao et al., 2022). We present an overview of the architecture, with full details in Appendix A. The model contains 70 layers of transformer decoder blocks defined as follows: h ̄l = hl−1 + SA(LN(hl−1)) hl = h ̄l + FFN(LN(h ̄l)) where SA is multi-head self-attention, LN is layer-normalization, and FFN is a feed-forward network with 1-hidden layer. Inside FFN, the non-linear function is GELU (Hendrycks and Gimpel, 2016). ALiBi positional encoding is applied through additive biases at the self- attention component of the transformer network (Le Scao et al., 2022). The input token embeddings are tied to the linear mapping before the final softmax. Following Le Scao et al. (2022) and first used in Dettmers et al. (2022), the model has an additional layer normalization after token embeddings, formally: h ̄1 = LNem(h0) + SA(LN(LNem(h0))), where h0 is the initial token embedding and LNem is the new component of embedding layer- normalization. Notice that the second term includes two consecutive layer-normalizations.\",\n",
        "    \"We train BloombergGPT, a 50 billion parameter language model that supports a wide range of tasks within the financial industry. Rather than building a general-purpose LLM, or a small LLM exclusively on domain-specific data, we take a mixed approach. General 3 models cover many domains, are able to perform at a high level across a wide variety of tasks, and obviate the need for specialization during training time. However, results from existing domain-specific models show that general models cannot replace them. At Bloomberg, we support a very large and diverse set of tasks, well served by a general model, but the vast majority of our applications are within the financial domain, better served by a specific model. For that reason, we set out to build a model that achieves best-in-class results on financial benchmarks, while also maintaining competitive performance on general-purpose LLM benchmarks.\",\n",
        "    \"Hardware Stack. We use the Amazon SageMaker service provided by AWS to train and evaluate BloombergGPT. We use the latest version available at the time of training and train on a total of 64 p4d.24xlarge instances. Each p4d.24xlarge instance has 8 NVIDIA 40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) and NVIDIA GPUDirect using AWS Elastic Fabric Adapter (EFA) inter-node connections (400 Gb/s). This yields a total of 512 40GB A100 GPUs. For quick data access, we use Amazon FSX for Lustre, which supports up to 1000 MB/s read and write throughput per TiB storage unit.\"\n",
        "]"
      ],
      "metadata": {
        "id": "FTlDbTo3JKx1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answers generated by the model"
      ],
      "metadata": {
        "id": "Y8O9wrDFXdY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_general_content_answers = []\n",
        "\n",
        "for question in general_content_questions:\n",
        "  answer = qa_chain.invoke(input=question)['result']\n",
        "  models_general_content_answers.append(answer)\n",
        "\n",
        "for answer in models_general_content_answers:\n",
        "  print(answer)\n",
        "  print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM7Pp5qEXzNR",
        "outputId": "66152228-365d-4847-a09a-0b9d062f0b29"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BloombergGPT is a specialized large language model (LLM) designed for financial applications, comprising 50 billion parameters and trained on a comprehensive dataset called \"FinPile,\" which includes a mix of financial documents and general-purpose texts. The model aims to improve interactions with financial data, making it easier to generate valid Bloomberg Query Language (BQL) queries and assist journalists in crafting news articles and newsletters. \n",
            "\n",
            "BloombergGPT demonstrates strong performance on various financial tasks and outperforms many existing models, including some larger ones, while maintaining competitive results on general NLP benchmarks. Its development benefited from insights gained from existing models and emphasized the importance of high-quality training datasets. The article discusses the model's architecture, training methodology, and evaluation results, while also addressing ethical considerations and limitations in the use of such technology in the finance sector. Overall, BloombergGPT represents a significant advancement in applying NLP to finance.\n",
            "--------------------------------------------------\n",
            "BloombergGPT is a decoder-only causal language model that is based on the architecture of BLOOM. It consists of 70 layers of transformer decoder blocks. The model processes input text by taking previous tokens into account to predict the next token, which is characteristic of causal language models.\n",
            "\n",
            "Each layer in the model applies a self-attention mechanism, allowing the model to focus on different parts of the input sequence to capture relevant context. This is followed by a feed-forward neural network that further refines the processed information. The architecture is designed to handle a large vocabulary size and has a high hidden dimension, which contributes to its ability to generate and understand complex financial language.\n",
            "\n",
            "Overall, BloombergGPT is built to excel in financial tasks while also maintaining performance on general language tasks, thanks to its combination of domain-specific and general-purpose training data.\n",
            "--------------------------------------------------\n",
            "BloombergGPT is a large language model (LLM) specifically designed for the financial domain. It has 50 billion parameters and is trained on a comprehensive dataset called \"FinPile,\" which consists of a range of English financial documents, including news articles, filings, press releases, and web-scraped content. The model was developed to enhance interactions with financial data, making it more natural and accessible, particularly through its ability to transform natural language queries into valid Bloomberg Query Language (BQL).\n",
            "\n",
            "The main reasons for developing BloombergGPT include:\n",
            "\n",
            "1. To provide a specialized LLM for financial tasks, as no such model had been reported in the literature prior to its development.\n",
            "2. To improve the performance of financial applications, such as sentiment analysis, question answering, and generating news headlines.\n",
            "3. To leverage the extensive financial data available from Bloomberg to create a model that can outperform existing models on financial tasks while maintaining strong performance on general language processing tasks.\n",
            "--------------------------------------------------\n",
            "The hardware stack used to train BloombergGPT consists of the Amazon SageMaker service provided by AWS. It utilizes a total of 64 p4d.24xlarge instances, where each instance has 8 NVIDIA 40GB A100 GPUs, resulting in a total of 512 40GB A100 GPUs. These instances are interconnected using NVIDIA NVSwitch intra-node connections (600 GB/s) and NVIDIA GPUDirect with AWS Elastic Fabric Adapter (EFA) inter-node connections (400 Gb/s). For quick data access, Amazon FSX for Lustre is used, which supports up to 1000 MB/s read and write throughput per TiB storage unit.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chunks retrieved and their similarity score for each question:\n",
        "\n",
        "the langchain method similarity_search_with_score() takes the question as input to retrieve the most similar chunks. the higher the score, the better. it is useed by calculating cosine distances."
      ],
      "metadata": {
        "id": "TkEh9Grp7ER-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for quest in general_content_questions:\n",
        "  chunk_and_score = vectorstore.similarity_search_with_score(quest, k=4)\n",
        "\n",
        "  print(f'Question: {quest}')\n",
        "  print()\n",
        "\n",
        "  for i in range(min(3, len(chunk_and_score))):\n",
        "      chunk = chunk_and_score[i]\n",
        "      print(f'Chunk {i + 1}: {chunk[0].page_content}')\n",
        "      print()\n",
        "      print(f'Score: {chunk[1]}')\n",
        "      print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AilbMAPz7MDJ",
        "outputId": "064cc2a6-bca4-4234-d1cf-77c50e4e7ee3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Summarize the BloombergGPT article\n",
            "\n",
            "Chunk 1: specialization.\n",
            "Generation of Bloomberg Query Language. One use case for BloombergGPT is to\n",
            "make interactions with financial data more natural. An existing way to retrieve data is via\n",
            "the Bloomberg Query Language (BQL). BQL can be used to interact with different classes\n",
            "of securities, each with its own fields, functions, and parameters. BQL is an incredibly\n",
            "powerful but complex tool. As we show in Figure 4, BloombergGPT can be utilized to\n",
            "make BQL more accessible by transforming natural language queries into valid BQL.\n",
            "Suggestion of News Headlines. Other use cases that are well supported are in the news\n",
            "space. Since it is trained on many news articles, it can be used for many news applications\n",
            "and assist journalists in their day-to-day work. For example, when constructing newsletters,\n",
            "\n",
            "Score: 0.700397789478302\n",
            "--------------------------------------------------\n",
            "Chunk 2: results, such as OPT (Zhang et al., 2022a), did not match the performance of the original\n",
            "model. With the release of each subsequent model, the community’s understanding, ex-\n",
            "perience, and software tools increase. In developing BloombergGPT , we benefited from\n",
            "existing code developed as part of the BLOOM effort (Scao et al., 2022), showing that a\n",
            "moderately sized team can produce a competitive model on domain-specific data. We de-\n",
            "scribe our experiences training BloombergGPT in detail to support future training efforts\n",
            "and address each of the above topics.\n",
            "2 Dataset\n",
            "To train BloombergGPT , we construct “ FinPile ”, a comprehensive dataset consisting of\n",
            "a range of English financial documents including news, filings, press releases, web-scraped fi-\n",
            "\n",
            "Score: 0.729968249797821\n",
            "--------------------------------------------------\n",
            "Chunk 3: model that outperforms existing models on financial tasks by significant margins without\n",
            "sacrificing performance on general LLM benchmarks. Additionally, we explain our model-\n",
            "ing choices, training process, and evaluation methodology. We release Training Chronicles\n",
            "(Appendix C) detailing our experience in training BloombergGPT .\n",
            "Contents\n",
            "1 Introduction 3\n",
            "1.1BloombergGPT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n",
            "1.2 Broader Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n",
            "2 Dataset 5\n",
            "2.1 Financial Datasets (363B tokens – 51.27% of training) . . . . . . . . . . . . 7\n",
            "2.1.1 Web (298B tokens – 42.01% of training) . . . . . . . . . . . . . . . . 7\n",
            "2.1.2 News (38B tokens – 5.31% of training) . . . . . . . . . . . . . . . . . 7\n",
            "\n",
            "Score: 0.7502540349960327\n",
            "--------------------------------------------------\n",
            "Question: Explain BloombergGPT model architecture. in plain text, no formulas\n",
            "\n",
            "Chunk 1: Shape\n",
            "Number of Layers 70\n",
            "Number of Heads 40\n",
            "Vocabulary Size 131,072\n",
            "Hidden Dimension 7,680\n",
            "Total Parameters 50.6B\n",
            "Hyperparameters\n",
            "Max Learning Rate 6e-5\n",
            "Final Learning Rate 6e-6\n",
            "Learning Rate schedule cosine decay\n",
            "Gradient Clipping 0.3\n",
            "Training\n",
            "Tokens 569B\n",
            "Hardware 64 ×8 A100 40GB\n",
            "Throughput 32.5 sec/step\n",
            "avg. TFLOPs 102\n",
            "total FLOPS 2.36e23\n",
            "Table 4: A summary of the hyper-parameters and their values for BloombergGPT .\n",
            "3 Model\n",
            "3.1 Architecture\n",
            "Our model is a decoder-only causal language model based on BLOOM (Scao et al., 2022).\n",
            "We present an overview of the architecture, with full details in Appendix A.\n",
            "The model contains 70 layers of transformer decoder blocks defined as follows:\n",
            "¯hℓ=hℓ−1+ SA(LN( hℓ−1))\n",
            "hℓ=¯hℓ+ FFN(LN( ¯hℓ))\n",
            "\n",
            "Score: 0.5982486605644226\n",
            "--------------------------------------------------\n",
            "Chunk 2: results, such as OPT (Zhang et al., 2022a), did not match the performance of the original\n",
            "model. With the release of each subsequent model, the community’s understanding, ex-\n",
            "perience, and software tools increase. In developing BloombergGPT , we benefited from\n",
            "existing code developed as part of the BLOOM effort (Scao et al., 2022), showing that a\n",
            "moderately sized team can produce a competitive model on domain-specific data. We de-\n",
            "scribe our experiences training BloombergGPT in detail to support future training efforts\n",
            "and address each of the above topics.\n",
            "2 Dataset\n",
            "To train BloombergGPT , we construct “ FinPile ”, a comprehensive dataset consisting of\n",
            "a range of English financial documents including news, filings, press releases, web-scraped fi-\n",
            "\n",
            "Score: 0.7052018046379089\n",
            "--------------------------------------------------\n",
            "Chunk 3: larger models (hundreds of billions of parameters). While our goal for BloombergGPT\n",
            "was to be a best-in-class model for financial tasks, and we included general-purpose training\n",
            "data to support domain-specific training, the model has still attained abilities on general-\n",
            "purpose data that exceed similarly sized models, and in some cases match or outperform\n",
            "much larger models.\n",
            "30\n",
            "\n",
            "Score: 0.7105348706245422\n",
            "--------------------------------------------------\n",
            "Question: What is BloombergGPT and why was it developed?\n",
            "\n",
            "Chunk 1: specialization.\n",
            "Generation of Bloomberg Query Language. One use case for BloombergGPT is to\n",
            "make interactions with financial data more natural. An existing way to retrieve data is via\n",
            "the Bloomberg Query Language (BQL). BQL can be used to interact with different classes\n",
            "of securities, each with its own fields, functions, and parameters. BQL is an incredibly\n",
            "powerful but complex tool. As we show in Figure 4, BloombergGPT can be utilized to\n",
            "make BQL more accessible by transforming natural language queries into valid BQL.\n",
            "Suggestion of News Headlines. Other use cases that are well supported are in the news\n",
            "space. Since it is trained on many news articles, it can be used for many news applications\n",
            "and assist journalists in their day-to-day work. For example, when constructing newsletters,\n",
            "\n",
            "Score: 0.5725911259651184\n",
            "--------------------------------------------------\n",
            "Chunk 2: results, such as OPT (Zhang et al., 2022a), did not match the performance of the original\n",
            "model. With the release of each subsequent model, the community’s understanding, ex-\n",
            "perience, and software tools increase. In developing BloombergGPT , we benefited from\n",
            "existing code developed as part of the BLOOM effort (Scao et al., 2022), showing that a\n",
            "moderately sized team can produce a competitive model on domain-specific data. We de-\n",
            "scribe our experiences training BloombergGPT in detail to support future training efforts\n",
            "and address each of the above topics.\n",
            "2 Dataset\n",
            "To train BloombergGPT , we construct “ FinPile ”, a comprehensive dataset consisting of\n",
            "a range of English financial documents including news, filings, press releases, web-scraped fi-\n",
            "\n",
            "Score: 0.7177372574806213\n",
            "--------------------------------------------------\n",
            "Chunk 3: larger models (hundreds of billions of parameters). While our goal for BloombergGPT\n",
            "was to be a best-in-class model for financial tasks, and we included general-purpose training\n",
            "data to support domain-specific training, the model has still attained abilities on general-\n",
            "purpose data that exceed similarly sized models, and in some cases match or outperform\n",
            "much larger models.\n",
            "30\n",
            "\n",
            "Score: 0.7728402614593506\n",
            "--------------------------------------------------\n",
            "Question: Tell me about BloombergGPT hardware stack\n",
            "\n",
            "Chunk 1: model recovered on its own.\n",
            "Hardware Stack. We use the Amazon SageMaker service provided by AWS to train and\n",
            "evaluate BloombergGPT . We use the latest version available at the time of training and\n",
            "train on a total of 64 p4d.24xlarge instances. Each p4d.24xlarge instance has 8 NVIDIA\n",
            "40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) and NVIDIA\n",
            "GPUDirect using AWS Elastic Fabric Adapter (EFA) inter-node connections (400 Gb/s).\n",
            "This yields a total of 512 40GB A100 GPUs. For quick data access, we use Amazon FSX for\n",
            "Lustre, which supports up to 1000 MB/s read and write throughput per TiB storage unit.\n",
            "3.4 Large-scale Optimization\n",
            "To train BloombergGPT , which has a larger memory footprint than available GPU mem-\n",
            "\n",
            "Score: 0.7468510270118713\n",
            "--------------------------------------------------\n",
            "Chunk 2: Shape\n",
            "Number of Layers 70\n",
            "Number of Heads 40\n",
            "Vocabulary Size 131,072\n",
            "Hidden Dimension 7,680\n",
            "Total Parameters 50.6B\n",
            "Hyperparameters\n",
            "Max Learning Rate 6e-5\n",
            "Final Learning Rate 6e-6\n",
            "Learning Rate schedule cosine decay\n",
            "Gradient Clipping 0.3\n",
            "Training\n",
            "Tokens 569B\n",
            "Hardware 64 ×8 A100 40GB\n",
            "Throughput 32.5 sec/step\n",
            "avg. TFLOPs 102\n",
            "total FLOPS 2.36e23\n",
            "Table 4: A summary of the hyper-parameters and their values for BloombergGPT .\n",
            "3 Model\n",
            "3.1 Architecture\n",
            "Our model is a decoder-only causal language model based on BLOOM (Scao et al., 2022).\n",
            "We present an overview of the architecture, with full details in Appendix A.\n",
            "The model contains 70 layers of transformer decoder blocks defined as follows:\n",
            "¯hℓ=hℓ−1+ SA(LN( hℓ−1))\n",
            "hℓ=¯hℓ+ FFN(LN( ¯hℓ))\n",
            "\n",
            "Score: 0.9187332987785339\n",
            "--------------------------------------------------\n",
            "Chunk 3: Lustre, which supports up to 1000 MB/s read and write throughput per TiB storage unit.\n",
            "3.4 Large-scale Optimization\n",
            "To train BloombergGPT , which has a larger memory footprint than available GPU mem-\n",
            "ory on cloud instances, we rely on stage 3 of ZeRO optimization (Rajbhandari et al., 2020).\n",
            "We utilize the proprietary SageMaker Model Parallelism (SMP) library from AWS, which\n",
            "enables the automatic distribution of large models across multiple GPU devices and in-\n",
            "stances (Karakus et al., 2021). After experimenting with various techniques, we achieve\n",
            "102 TFLOPs on average and each training step takes 32.5 seconds. We find the following\n",
            "setup to be the best performing in our training.\n",
            "ZeRO Optimization (stage 3). ZeRO shards the training state (model parameters,\n",
            "\n",
            "Score: 0.9464210271835327\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metric 1: **BLEU**\n",
        "\n",
        "Blue takes into account how many n-grams are there in the model's answers compared to reference answers. On paramter weights, you can determine how much emphasis (weight) you want to give to 1-gram, 2-gram, and so on. Since only I only provided context and not actual, correct answers, I put more weights on 1-grams since it is important that some single, important words from the context appear in the model's answer.\n",
        "\n",
        "Between 0.3 and 0.5 is generally good"
      ],
      "metadata": {
        "id": "mV-KNniaV4Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nltk --quiet\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "#nltk.download('punkt')\n",
        "\n",
        "reference_tokens = [nltk.word_tokenize(answer) for answer in reference_general_content_answers]\n",
        "generated_tokens = [nltk.word_tokenize(answer) for answer in models_general_content_answers]\n",
        "\n",
        "bleu_scores = []\n",
        "\n",
        "for ref, gen in zip(reference_tokens, generated_tokens):\n",
        "\n",
        "    weights = (0.70, 0.20, 0.05, 0.05)\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "    reference_list = [ref]\n",
        "\n",
        "    bleu_score = sentence_bleu(\n",
        "        reference_list,\n",
        "        gen,\n",
        "        weights=weights,\n",
        "        smoothing_function=smoothing_function,\n",
        "        auto_reweigh=False\n",
        "    )\n",
        "\n",
        "    bleu_scores.append(bleu_score)\n",
        "\n",
        "for i, (score, question) in enumerate(zip(bleu_scores, general_content_questions)):\n",
        "    print(f'Question {i+1}: {question}')\n",
        "    print(f\"BLEU score: {score:.6f}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAFFqLBzVLDO",
        "outputId": "ed3534d1-0d4e-4930-834a-340f973da1e9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: Summarize the BloombergGPT article\n",
            "BLEU score: 0.173698\n",
            "--------------------------------------------------\n",
            "Question 2: Explain BloombergGPT model architecture. in plain text, no formulas\n",
            "BLEU score: 0.155016\n",
            "--------------------------------------------------\n",
            "Question 3: What is BloombergGPT and why was it developed?\n",
            "BLEU score: 0.161842\n",
            "--------------------------------------------------\n",
            "Question 4: Tell me about BloombergGPT hardware stack\n",
            "BLEU score: 0.703667\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metric 2: **ROUGE**\n",
        "\n",
        "It looks how many of unigram. bigram, and longer sequences the generated contend is covering from the refernece answer (context in this case). It is more suitable for the purpose of this model, since lack of correct reference answers."
      ],
      "metadata": {
        "id": "EkcWKfG_fj9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install rouge-score --quiet\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "\n",
        "#nltk.download('punkt')\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "reference = reference_general_content_answers\n",
        "generated = models_general_content_answers\n",
        "\n",
        "rouge_scores = []\n",
        "\n",
        "for ref, gen in zip(reference, generated):\n",
        "    score = scorer.score(ref, gen)\n",
        "    rouge_scores.append(score)\n",
        "\n",
        "for i, (score, question) in enumerate(zip(rouge_scores, general_content_questions)):\n",
        "    print(f'Question {i+1}: {question}')\n",
        "    print()\n",
        "    print(f\"ROUGE score for question {i + 1}:\")\n",
        "    print(f\"ROUGE-1: {score['rouge1'].fmeasure:.4f}\")\n",
        "    print(f\"ROUGE-2: {score['rouge2'].fmeasure:.4f}\")\n",
        "    print(f\"ROUGE-L: {score['rougeL'].fmeasure:.4f}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoouTG3nb63u",
        "outputId": "831cc15e-a0fd-42de-c33b-9cecff3370de"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: Summarize the BloombergGPT article\n",
            "\n",
            "ROUGE score for question 1:\n",
            "ROUGE-1: 0.4587\n",
            "ROUGE-2: 0.1108\n",
            "ROUGE-L: 0.2141\n",
            "--------------------------------------------------\n",
            "Question 2: Explain BloombergGPT model architecture. in plain text, no formulas\n",
            "\n",
            "ROUGE score for question 2:\n",
            "ROUGE-1: 0.3494\n",
            "ROUGE-2: 0.1212\n",
            "ROUGE-L: 0.2048\n",
            "--------------------------------------------------\n",
            "Question 3: What is BloombergGPT and why was it developed?\n",
            "\n",
            "ROUGE score for question 3:\n",
            "ROUGE-1: 0.3934\n",
            "ROUGE-2: 0.0726\n",
            "ROUGE-L: 0.2164\n",
            "--------------------------------------------------\n",
            "Question 4: Tell me about BloombergGPT hardware stack\n",
            "\n",
            "ROUGE score for question 4:\n",
            "ROUGE-1: 0.8458\n",
            "ROUGE-2: 0.6633\n",
            "ROUGE-L: 0.6965\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metric 3: **BERT score**\n",
        "\n",
        "Use the contextual embedding from BERT to compare semantic meaning between referenc/context and model's answers, considering tokens in both answers. It ranges from 0 to 1. The closer to 1, more similar in meaning they are."
      ],
      "metadata": {
        "id": "2rlswwb8O60o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bert_score --quiet\n",
        "\n",
        "from bert_score import score\n",
        "\n",
        "references = reference_general_content_answers\n",
        "models_answers = models_general_content_answers\n",
        "\n",
        "# Compute BERTScore\n",
        "P, R, F1 = score(models_answers, references, lang='en', verbose=False)\n",
        "\n",
        "\n",
        "for i in range(len(references)):\n",
        "    print(f'Question {i+1}: {general_content_questions[i]}')\n",
        "    print()\n",
        "    print(f\"Generated Answer {i + 1}:\")\n",
        "    print(f\"Precision: {P[i]:.4f}\")\n",
        "    print(f\"Recall: {R[i]:.4f}\")\n",
        "    print(f\"F1: {F1[i]:.4f}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4qBoeZsO6ct",
        "outputId": "78a195a2-f9fb-4de3-9d82-a34829920fbc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: Summarize the BloombergGPT article\n",
            "\n",
            "Generated Answer 1:\n",
            "Precision: 0.8734\n",
            "Recall: 0.8650\n",
            "F1: 0.8692\n",
            "--------------------------------------------------\n",
            "Question 2: Explain BloombergGPT model architecture. in plain text, no formulas\n",
            "\n",
            "Generated Answer 2:\n",
            "Precision: 0.8546\n",
            "Recall: 0.7881\n",
            "F1: 0.8200\n",
            "--------------------------------------------------\n",
            "Question 3: What is BloombergGPT and why was it developed?\n",
            "\n",
            "Generated Answer 3:\n",
            "Precision: 0.8541\n",
            "Recall: 0.8626\n",
            "F1: 0.8583\n",
            "--------------------------------------------------\n",
            "Question 4: Tell me about BloombergGPT hardware stack\n",
            "\n",
            "Generated Answer 4:\n",
            "Precision: 0.9583\n",
            "Recall: 0.9498\n",
            "F1: 0.9540\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric 4: **Cosine Similarity**\n",
        "\n",
        "It embeds the whole reference/context answer and the whole generated answer and then compara its semanting similarity by assessing how close the cosines are. It ranges from 0 to 1. The closer to 1, more similar in meaning they are."
      ],
      "metadata": {
        "id": "WFXtJmWSrCxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "reference_answers = reference_general_content_answers\n",
        "model_answers = models_general_content_answers\n",
        "\n",
        "similarity_scores = []\n",
        "\n",
        "for ref, gen in zip(reference_answers, model_answers):\n",
        "    # Tokenize\n",
        "    inputs_ref = tokenizer(ref, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "    # Context. Embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs_ref = model(**inputs_ref)\n",
        "    ref_embedding = outputs_ref.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "    # Tokenize\n",
        "    inputs_gen = tokenizer(gen, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "    # Context. Embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs_gen = model(**inputs_gen)\n",
        "    gen_embedding = outputs_gen.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "    similarity_score = cosine_similarity(ref_embedding, gen_embedding)\n",
        "    similarity_scores.append(similarity_score[0][0])\n",
        "\n",
        "for i, (score, question) in enumerate(zip(similarity_scores, general_content_questions)):\n",
        "    print(f'Question {i+1}: {question}')\n",
        "    print(f\"Cosine Similarity between both answers {i + 1}: {score:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7EZRRPJl0bu",
        "outputId": "5ae63d38-b98d-4698-89da-0a3395aa2a73"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: Summarize the BloombergGPT article\n",
            "Cosine Similarity between both answers 1: 0.8349\n",
            "--------------------------------------------------\n",
            "Question 2: Explain BloombergGPT model architecture. in plain text, no formulas\n",
            "Cosine Similarity between both answers 2: 0.7369\n",
            "--------------------------------------------------\n",
            "Question 3: What is BloombergGPT and why was it developed?\n",
            "Cosine Similarity between both answers 3: 0.8595\n",
            "--------------------------------------------------\n",
            "Question 4: Tell me about BloombergGPT hardware stack\n",
            "Cosine Similarity between both answers 4: 0.9034\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}